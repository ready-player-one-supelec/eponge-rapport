\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}
\fancyhead[R]{\textit{Conclusion}}

Plusieurs objectifs ont été remplis lors de ce projet. Tout d'abord nous avons pu acquérir les bases sur les réseaux neuronaux. L'étude de la documentation scientifique nous a permis de comprendre précisément comment fonctionne un perceptron et d'en coder un nous même en Python. Nous avons été confrontés aux questions telles que le choix de la méthode de descente du gradient, de la fonction d'erreur ou encore de la taille des batchs. Nous avons ainsi pu apprendre à calibrer notre perceptron sur des problèmes simples comme la fonction XOR avant de s'attaquer à des problèmes plus complexes tel MNIST.

Ce problème lié à la reconnaissance d'images nous a naturellement conduit à étudier les réseaux de neurones à convolution. Encore une fois, l'étude de problèmes simples tels que la reconnaissance d'une croix nous ont permis d'appréhender facilement les différents paramètres des réseaux convolutifs. Le choix a alors été fait de ne pas coder nous même le réseau convolutif mais d'utiliser la bibliothèque Python TensorFlow. Le réseau développé en TensorFlow est en effet plus efficace qu'un réseau codé à la main. TensorFLow nous a également permis de gagner le temps que nous aurions passé sur l'implémentation du réseau. Notre projet était en effet d'une durée d'un an, et nous devions nous concentrer sur le c\oe ur du sujet, le Q-Learning.

Nous avons pu nous familiariser sur le Q-Learning en résolvant des problèmes simples, tel que le jeu des bâtonnets ou le jeu du labyrinthe. Notre but final étant de jouer au jeu Pong, nous ne pouvions nous contenter du Q-Learning. Nous avons dû utiliser le Deep Q-Learning car le jeu est beaucoup trop complexe. Nous avons alors fait appel à nos connaissances sur les réseaux convolutifs pour remplacer la Q-table utilisée dans le Q-Learning. Nous avons alors été confronté à de nombreux problèmes pour résoudre le jeu Pong : utilisation de deux réseaux, choix de l'optimiseur, configuration du réseau, utilisation de la mémoire, etc. Le plus grand problème a cependant été les capacités de calcul pour que le réseau apprenne à résoudre le jeu. Nous avons réussi à gagner des parties mais il nous est impossible de réaliser des centaines de runs pour tester différents paramétrages du réseau. Nous avons donc décidé de nous replier sur un jeu plus simple donc moins gourmand en ressources. Nous avons choisi le jeu CartPole sur lequel nous avons étudié l'évolution de taux de succès en fonction du taux d'entraînement. Notre algorithme était stable et est arrivé à résoudre le jeu dans un temps raisonnable. Nous avons enfin utilisé notre algorithme pour résoudre avec succès le jeu Flappy Bird.
\\

Ce projet nous a montré que le Deep Q-Learning permettait de résoudre des problèmes complexes comme le jeu Pong mais qu'il nécessitait d'énormes puissances de calcul. Par ailleurs, le processus d'apprentissage reste assez opaque et parfois instable.